rule bwa:
    # Do the alignment, read paring, introduction of the read groups, sam-import and sorting on a pipe. Paired-end flavour
    input:
        forward=lambda wildcards: "%s%s%s/%s/%s_R1.fastq.gz" % (wildcards.prefix, config['dirs']['intermediate'], config['stepnames']['xenograft_check'], get_kind_of_run(wildcards, SAMPLESHEETS, config), wildcards.sample),
        reverse=lambda wildcards: get_reverse_file("%s%s%s/Paired/%s_R2.fastq.gz" % (wildcards.prefix, config['dirs']['intermediate'], config['stepnames']['xenograft_check'], wildcards.sample), wildcards, SAMPLESHEETS, config),
        references=lambda wildcards: ["%s%s%s%s" % (wildcards.prefix, config['dirs']['references'], get_reference_genome(wildcards.sample, SAMPLESHEETS, config)['file'], ending) for ending in config['endings_bwa']]
    output:
        "{prefix}%s%s/{sample}.new.Hg38.sam" % (config['dirs']['intermediate'], config['stepnames']['map'])
    log:
        bwa_mem="{prefix}%s%s/{sample}.bwa_mem.log" % (config['dirs']['logs'], config['stepnames']['map']),
        #sam_view="{prefix}%s%s/{sample}.samtools_view.log" % (config['dirs']['logs'], config['stepnames']['map']),
        #sam_sort="{prefix}%s%s/{sample}.samtools_sort.log" % (config['dirs']['logs'], config['stepnames']['map']),
    benchmark:
        "{prefix}%s%s/{sample}.benchmark" % (config['dirs']['benchmarks'], config['stepnames']['map'])
    conda:
        "envs/spike_map.yaml"
    threads:
        6
    params:
        header=lambda wildcards: get_bwa_mem_header(wildcards.sample, SAMPLESHEETS, config)
    shell:
        # wow: I just saw that the number of threads has an impact on the results!
        # thus, for the moment, we should use the same number of threads as Michael's
        # original pipeline, which is 3
        'bwa mem -t {threads} -v 2 -M'
        '{params.header}'
        ' {input.references[0]}'
        ' {input.forward} {input.reverse} 2> {log.bwa_mem} > {output}'


##### convert to sorted bam, index & cleanup
rule sam_cleanup:
    input:
        "{prefix}%s%s/{sample}.new.Hg38.sam" % (config['dirs']['intermediate'], config['stepnames']['map'])
    output:
        "{prefix}%s%s/{sample}.new.Hg38.srt.bam" % (config['dirs']['intermediate'], config['stepnames']['map']),
        #bai="{prefix}%s%s/{sample}.nodup.srt.bai" % (config['dirs']['intermediate'], config['stepnames']['nodup']),
        #metric="{prefix}%s%s/{sample}.nodup.srt.bam.metrics" % (config['dirs']['intermediate'], config['stepnames']['nodup']),
    log:
        "{prefix}%s%s/{sample}.log" % (config['dirs']['logs'], config['stepnames']['map'])
    conda:
        "envs/spike_map.yaml"
    threads:
        1  # since this picard tools function doesn't support multithreading :-(
    shell:
        "java"
        " -Xmx4g"
        " -XX:ParallelGCThreads={threads}"
        " -jar ${{CONDA_PREFIX}}/share/picard-2.23.8-0/picard.jar"
        " SortSam"
        " I={input}"
        " O={output}"
        " SORT_ORDER=coordinate"
        " CREATE_INDEX=true"
        " VALIDATION_STRINGENCY=LENIENT"
        " 2> {log}"
	# delete input file after process finished
        #" && rm -v -f {input}"	

rule MBC_remove_duplicates:
    # Remove duplicates reads based on MBC
    input:
        Sbam="{prefix}%s%s/{sample}.new.Hg38.srt.bam" % (config['dirs']['intermediate'], config['stepnames']['map']),
	#Sbam="{prefix}%s%s/{sample}.Hg38.nodup.srt.bam" % (config['dirs']['intermediate'], config['stepnames']['nodup']),
	MBC="{prefix}%s%s/{sample}_R2.fastq.gz" % (config['dirs']['intermediate'], config['stepnames']['rejoin_samples']),
        bin_LocatIt="{prefix}%slocatit-2.0.5.jar" % (config['dirs']['references']),
        agilent=lambda wildcards: '%s%s%s' % (wildcards.prefix, config['dirs']['references'], get_reference_exometrack(wildcards.sample, SAMPLESHEETS, config, returnfield="agilent_coverage_file")),
	#exometrack=lambda wildcards: "%s%s%s" % (config['dirs']['prefix'], config['dirs']['references'], get_reference_exometrack(wildcards.sample, SAMPLESHEETS, config)),
    output:
        "{prefix}%s%s/{sample}.KB0345_f_hg_38_MBC_srt.bam" % (config['dirs']['intermediate'], config['stepnames']['UMI'])
    log:
        "{prefix}%s%s/{sample}_2.log" % (config['dirs']['logs'], config['stepnames']['UMI'])
    benchmark:
        "{prefix}%s%s/{sample}.benchmark" % (config['dirs']['benchmarks'], config['stepnames']['UMI'])
    conda:
        "envs/spike_map.yaml"
    threads:
        1  # since this tool function doesn't support multithreading
    shell:
        "java"
        " -Xmx12G"
        " -jar {input.bin_LocatIt}"
        " -q 25"
        " -m 1"
        " -C"
        " -i"
        #" -r"
	" -R"
	" -S"
        " -l {input.agilent}"
        " -o {output}"
	" {input.Sbam}"
        " {input.MBC}"
	" > {log} 2>&1"
'''

rule remove_pcr_duplicates:
    # Remove PCR duplicates from the reads
    input:
        "{prefix}%s%s/{sample}.new.Hg38.srt.bam" % (config['dirs']['intermediate'], config['stepnames']['map']),
	#"{prefix}%s%s/{sample}.MBC.new.Hg38.srt.bam" % (config['dirs']['intermediate'], config['stepnames']['UMI'])
    output:
        bam="{prefix}%s%s/{sample}.new.Hg38.nodup.srt.bam" % (config['dirs']['intermediate'], config['stepnames']['nodup']),
        bai="{prefix}%s%s/{sample}.new.Hg38.nodup.srt.bai" % (config['dirs']['intermediate'], config['stepnames']['nodup']),
        metric="{prefix}%s%s/{sample}.new.Hg38.nodup.srt.bam.metrics" % (config['dirs']['intermediate'], config['stepnames']['nodup']),
    log:
        "{prefix}%s%s/{sample}.log" % (config['dirs']['logs'], config['stepnames']['nodup'])
    benchmark:
        "{prefix}%s%s/{sample}.benchmark" % (config['dirs']['benchmarks'], config['stepnames']['nodup'])
    conda:
        "envs/spike_map.yaml"
    threads:
        1  # since this picard tools function doesn't support multithreading :-(
    shell:
        "java"
        " -Xmx8G"
        " -XX:ParallelGCThreads={threads}"
        " -jar ${{CONDA_PREFIX}}/share/picard-2.23.8-0/picard.jar"
        " MarkDuplicates"
        " INPUT={input}"
        " METRICS_FILE={output.metric}"
        " REMOVE_DUPLICATES=true"
        " ASSUME_SORTED=true"
        " VALIDATION_STRINGENCY=LENIENT"
        " CREATE_INDEX=true"
        " OUTPUT={output.bam}"
        " > {log} 2>&1"


rule MBC_remove_duplicates:
    # Remove duplicates reads based on MBC
    input:
        Sbam="{prefix}%s%s/{sample}.new.Hg38.srt.bam" % (config['dirs']['intermediate'], config['stepnames']['map']),
        #Sbam="{prefix}%s%s/{sample}.new.Hg38.nodup.srt.bam" % (config['dirs']['intermediate'], config['stepnames']['nodup']),
        MBC="{prefix}%s%s/{sample}_R2.fastq.gz" % (config['dirs']['intermediate'], config['stepnames']['rejoin_samples']),
        bin_LocatIt="{prefix}%slocatit-2.0.5.jar" % (config['dirs']['references']),
        agilent=lambda wildcards: '%s%s%s' % (wildcards.prefix, config['dirs']['references'], get_reference_exometrack(wildcards.sample, SAMPLESHEETS, config, returnfield="agilent_coverage_file")),
        #exometrack=lambda wildcards: "%s%s%s" % (config['dirs']['prefix'], config['dirs']['references'], get_reference_exometrack(wildcards.sample, SAMPLESHEETS, config)),
    output:
        "{prefix}%s%s/{sample}.new.Hg38.MBC_no_r_.srt.bam" % (config['dirs']['intermediate'], config['stepnames']['UMI'])
    log:
        "{prefix}%s%s/{sample}_2.log" % (config['dirs']['logs'], config['stepnames']['UMI'])
    benchmark:
        "{prefix}%s%s/{sample}.benchmark" % (config['dirs']['benchmarks'], config['stepnames']['UMI'])
    conda:
        "envs/spike_map.yaml"
    threads:
        1  # since this tool function doesn't support multithreading
    shell:
        "java"
        " -Xmx12G"
        " -jar {input.bin_LocatIt}"
        " -q 25"
        " -m 1"
        " -C"
        " -i"
        #" -r"
        " -R"
        " -S"
        " -l {input.agilent}"
        " -o {output}"
        " {input.Sbam}"
        " {input.MBC}"
        " > {log} 2>&1"


rule remove_pcr_duplicates:
    # Remove PCR duplicates from the reads
    input:
	"{prefix}%s%s/{sample}.MBC.new.Hg38.srt.bam" % (config['dirs']['intermediate'], config['stepnames']['UMI'])
	#"{prefix}%s%s/{sample}.new.Hg38.srt.bam" % (config['dirs']['intermediate'], config['stepnames']['map']),
    output:
        bam="{prefix}%s%s/{sample}.MBC.new.Hg38.nodup.srt.bam" % (config['dirs']['intermediate'], config['stepnames']['nodup']),
        bai="{prefix}%s%s/{sample}.MBC.new.Hg38.nodup.srt.bai" % (config['dirs']['intermediate'], config['stepnames']['nodup']),
        metric="{prefix}%s%s/{sample}.MBC.new.Hg38.nodup.srt.bam.metrics" % (config['dirs']['intermediate'], config['stepnames']['nodup']),
    log:
        "{prefix}%s%s/{sample}.log" % (config['dirs']['logs'], config['stepnames']['nodup'])
    benchmark:
        "{prefix}%s%s/{sample}.benchmark" % (config['dirs']['benchmarks'], config['stepnames']['nodup'])
    conda:
        "envs/spike_map.yaml"
    threads:
        1  # since this picard tools function doesn't support multithreading :-(
    shell:
        "java"
        " -Xmx8G"
        " -XX:ParallelGCThreads={threads}"
        " -jar ${{CONDA_PREFIX}}/share/picard-2.23.8-0/picard.jar"
        " MarkDuplicates"
        " INPUT={input}"
        " METRICS_FILE={output.metric}"
        " REMOVE_DUPLICATES=true"
        " ASSUME_SORTED=true"
        " VALIDATION_STRINGENCY=LENIENT"
        " CREATE_INDEX=true"
        " OUTPUT={output.bam}"
        " > {log} 2>&1"
'''
